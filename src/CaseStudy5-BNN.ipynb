{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Case Study 5: Bayesian Neural Network\n",
        "\n",
        "Adapted from https://num.pyro.ai/en/stable/examples/bnn.html , we first see the NumPyro implementation and then SOGA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/rominadoz/micromamba/envs/simple_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from sogaPreprocessor import *\n",
        "from producecfg import *\n",
        "from libSOGA import *\n",
        "from time import time\n",
        "\n",
        "torch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sample: 100%|██████████| 3000/3000 [00:01<00:00, 2557.52it/s, 31 steps of size 1.12e-01. acc. prob=0.81]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
            "  prec_obs      1.09      0.38      1.03      0.49      1.67     25.41      1.05\n",
            "   w1[0,0]     -0.14      0.93     -0.11     -1.70      1.33    929.04      1.00\n",
            "   w1[0,1]     -0.29      0.93     -0.35     -1.62      1.32    128.60      1.02\n",
            "   w1[1,0]      0.21      1.07      0.22     -1.37      2.26     47.76      1.02\n",
            "   w1[1,1]      0.19      1.17      0.09     -1.48      2.19     31.67      1.04\n",
            "   w2[0,0]     -0.09      0.94     -0.08     -1.74      1.36    922.71      1.00\n",
            "   w2[0,1]      0.05      1.05     -0.00     -1.63      1.83     45.04      1.03\n",
            "   w2[1,0]      0.14      1.17      0.07     -1.75      1.83     33.45      1.04\n",
            "   w2[1,1]     -0.23      1.04     -0.21     -1.91      1.45    135.33      1.00\n",
            "   w3[0,0]     -0.26      1.05     -0.21     -2.01      1.47     48.08      1.03\n",
            "   w3[1,0]      0.02      1.00     -0.01     -1.59      1.56     84.05      1.03\n",
            "\n",
            "Number of divergences: 43\n",
            "\n",
            "MCMC elapsed time: 3.5735161304473877\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "from jax import vmap\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "\n",
        "import numpyro\n",
        "from numpyro import handlers\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS\n",
        "\n",
        "matplotlib.use(\"Agg\")  # noqa: E402\n",
        "\n",
        "\n",
        "# the non-linearity we use in our neural network\n",
        "def nonlin(x):\n",
        "    return jax.nn.relu(x)\n",
        "\n",
        "\n",
        "# a two-layer bayesian neural network with computational flow\n",
        "# given by D_X => D_H => D_H => D_Y where D_H is the number of\n",
        "# hidden units. (note we indicate tensor dimensions in the comments)\n",
        "def model(X, Y, D_H, D_Y=1):\n",
        "    N, D_X = X.shape\n",
        "\n",
        "    # sample first layer (we put unit normal priors on all weights)\n",
        "    w1 = numpyro.sample(\"w1\", dist.Normal(jnp.zeros((D_X, D_H)), jnp.ones((D_X, D_H))))\n",
        "    assert w1.shape == (D_X, D_H)\n",
        "    z1 = nonlin(jnp.matmul(X, w1))  # <= first layer of activations\n",
        "    assert z1.shape == (N, D_H)\n",
        "\n",
        "    # sample second layer\n",
        "    w2 = numpyro.sample(\"w2\", dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H))))\n",
        "    assert w2.shape == (D_H, D_H)\n",
        "    z2 = nonlin(jnp.matmul(z1, w2))  # <= second layer of activations\n",
        "    assert z2.shape == (N, D_H)\n",
        "\n",
        "    # sample final layer of weights and neural network output\n",
        "    w3 = numpyro.sample(\"w3\", dist.Normal(jnp.zeros((D_H, D_Y)), jnp.ones((D_H, D_Y))))\n",
        "    assert w3.shape == (D_H, D_Y)\n",
        "    z3 = jnp.matmul(z2, w3)  # <= output of the neural network\n",
        "    assert z3.shape == (N, D_Y)\n",
        "\n",
        "    if Y is not None:\n",
        "        assert z3.shape == Y.shape\n",
        "\n",
        "    # we put a prior on the observation noise\n",
        "    prec_obs = numpyro.sample(\"prec_obs\", dist.Normal(0., 1.0))  #Originally Gamma(3.0, 1.0)\n",
        "    sigma_obs = 1.0 / jnp.sqrt(prec_obs)\n",
        "\n",
        "    # observe data\n",
        "    with numpyro.plate(\"data\", N):\n",
        "        # note we use to_event(1) because each observation has shape (1,)\n",
        "        numpyro.sample(\"Y\", dist.Normal(z3, sigma_obs).to_event(1), obs=Y)\n",
        "\n",
        "\n",
        "# helper function for HMC inference\n",
        "def run_inference(model, rng_key, X, Y, D_H):\n",
        "    start = time.time()\n",
        "    kernel = NUTS(model)\n",
        "    mcmc = MCMC(\n",
        "        kernel,\n",
        "        num_warmup=1000,\n",
        "        num_samples=2000,\n",
        "        num_chains=1,\n",
        "        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n",
        "    )\n",
        "    mcmc.run(rng_key, X, Y, D_H)\n",
        "    mcmc.print_summary()\n",
        "    print(\"\\nMCMC elapsed time:\", time.time() - start)\n",
        "    return mcmc.get_samples()\n",
        "\n",
        "\n",
        "# helper function for prediction\n",
        "def predict(model, rng_key, samples, X, D_H):\n",
        "    model = handlers.substitute(handlers.seed(model, rng_key), samples)\n",
        "    # note that Y will be sampled in the model because we pass Y=None here\n",
        "    model_trace = handlers.trace(model).get_trace(X=X, Y=None, D_H=D_H)\n",
        "    return model_trace[\"Y\"][\"value\"]\n",
        "\n",
        "\n",
        "# create artificial regression dataset\n",
        "def get_data(N=20, D_X=3, sigma_obs=0.05, N_test=500):\n",
        "    D_Y = 1  # create 1d outputs\n",
        "    np.random.seed(0)\n",
        "    X = jnp.linspace(-1, 1, N)\n",
        "    X = jnp.power(X[:, np.newaxis], jnp.arange(D_X))\n",
        "    W = 0.5 * np.random.randn(D_X)\n",
        "    Y = jnp.dot(X, W) + 0.5 * jnp.power(0.5 + X[:, 1], 2.0) * jnp.sin(4.0 * X[:, 1])\n",
        "    Y += sigma_obs * np.random.randn(N)\n",
        "    Y = Y[:, np.newaxis]\n",
        "    Y -= jnp.mean(Y)\n",
        "    Y /= jnp.std(Y)\n",
        "\n",
        "    assert X.shape == (N, D_X)\n",
        "    assert Y.shape == (N, D_Y)\n",
        "\n",
        "    X_test = jnp.linspace(-1.3, 1.3, N_test)\n",
        "    X_test = jnp.power(X_test[:, np.newaxis], jnp.arange(D_X))\n",
        "\n",
        "    return X, Y, X_test\n",
        "\n",
        "\n",
        "args = [20, 2, 2]\n",
        "N, D_X, D_H = args\n",
        "X, Y, X_test = get_data(N=N, D_X=D_X)\n",
        "\n",
        "# do inference\n",
        "rng_key, rng_key_predict = random.split(random.PRNGKey(0))\n",
        "samples = run_inference(model, rng_key, X, Y, D_H)\n",
        "\n",
        "# predict Y_test at inputs X_test\n",
        "vmap_args = (\n",
        "    samples,\n",
        "    random.split(rng_key_predict, 2000 * 1),\n",
        ")\n",
        "predictions = vmap(\n",
        "    lambda samples, rng_key: predict(model, rng_key, samples, X_test, D_H)\n",
        ")(*vmap_args)\n",
        "predictions = predictions[..., 0]\n",
        "\n",
        "# compute mean prediction and confidence interval around median\n",
        "mean_prediction = jnp.mean(predictions, axis=0)\n",
        "percentiles = np.percentile(predictions, [5.0, 95.0], axis=0)\n",
        "\n",
        "# make plots\n",
        "fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n",
        "\n",
        "# plot training data\n",
        "ax.plot(X[:, 1], Y[:, 0], \"kx\")\n",
        "# plot 90% confidence level of predictions\n",
        "ax.fill_between(\n",
        "    X_test[:, 1], percentiles[0, :], percentiles[1, :], color=\"lightblue\"\n",
        ")\n",
        "# plot mean prediction\n",
        "ax.plot(X_test[:, 1], mean_prediction, \"blue\", ls=\"solid\", lw=2.0)\n",
        "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 90% CI\")\n",
        "\n",
        "plt.savefig(\"bnn_plot.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.0, -0.8947368860244751, -0.7894736528396606, -0.6842105388641357, -0.5789473056793213, -0.4736841917037964, -0.3684210777282715, -0.2631578743457794, -0.15789473056793213, -0.05263158679008484, 0.05263161659240723, 0.15789473056793213, 0.26315784454345703, 0.3684210777282715, 0.4736841917037964, 0.5789474248886108, 0.6842105388641357, 0.7894736528396606, 0.8947368860244751, 1.0]\n",
            "(20,)\n"
          ]
        }
      ],
      "source": [
        "print((X[:,1]).tolist())\n",
        "print(X[:,1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def optimize(params_dict, loss_function, y, cfg, steps=500):\n",
        "    optimizer = torch.optim.Adam([params_dict[key] for key in params_dict.keys()], lr=1)\n",
        "\n",
        "    total_start = time()\n",
        "\n",
        "    for i in range(steps):\n",
        "\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        \n",
        "        # loss\n",
        "        current_dist = start_SOGA(cfg, params_dict)\n",
        "\n",
        "        loss = loss_function(y, current_dist)\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward(retain_graph=True)\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if i % 10 == 0:\n",
        "            out = ''\n",
        "            for key in params_dict.keys():\n",
        "                out = out + key + ': ' + str(params_dict[key].item()) + ' '\n",
        "            out = out + f\" loss: {loss.item()}\"\n",
        "            print(out)\n",
        "\n",
        "    total_end = time()\n",
        "\n",
        "    print('Optimization performed in ', round(total_end-total_start, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true, dist):\n",
        "    return torch.mean((y_true - dist.gm.mean()) ** 2)\n",
        "\n",
        "def mean_squared_error_bayes(y_true, dist):\n",
        "    #This works for the means but of course not for the variances\n",
        "    return torch.mean((y_true - dist.gm.mean()[:-2]) ** 2)\n",
        "\n",
        "def neg_log_likelihood(y_true, dist):\n",
        "    #Calculate the log-likelihood of the data given the distribution\n",
        "    neg_log_likelihood = 0\n",
        "    for i in range(len(dist.gm.mean())-2):\n",
        "        neg_log_likelihood -= torch.log(dist.gm.marg_pdf(y_true[i].unsqueeze(0), i))\n",
        "    return neg_log_likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.0, -0.8947368860244751, -0.7894736528396606, -0.6842105388641357, -0.5789473056793213, -0.4736841917037964, -0.3684210777282715, -0.2631578743457794, -0.15789473056793213, -0.05263158679008484, 0.05263161659240723, 0.15789473056793213, 0.26315784454345703, 0.3684210777282715, 0.4736841917037964, 0.5789474248886108, 0.6842105388641357, 0.7894736528396606, 0.8947368860244751, 1.0]\n",
            "(20,)\n"
          ]
        }
      ],
      "source": [
        "print((X[:,1]).tolist())\n",
        "print(X[:,1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "compiledFile=compile2SOGA('../programs/SOGA/Optimization/Case Studies/bnn3.soga')\n",
        "cfg = produce_cfg(compiledFile)\n",
        "\n",
        "#pars = {'mu100':0., 'sigma100':1., 'mu101':0., 'sigma101':1.,'mu110':0., 'sigma110':1.,'mu111':0., 'sigma111':1.,'mu200':0., 'sigma200':1.,\n",
        "        #'mu201':0., 'sigma201':1.,'mu210':0., 'sigma210':1.,'mu211':0., 'sigma211':1.,'mu300':0., 'sigma300':1.,'mu310':0., 'sigma310':1.,}\n",
        "\n",
        "\n",
        "pars = {'mu100':0., 'sigma100':1., 'mu101':0., 'sigma101':1.,'mu110':0., 'sigma110':1.,'mu111':0., 'sigma111':1.,'mu300':0., 'sigma300':1.,'mu310':0., 'sigma310':1.,}\n",
        "params_dict = {}\n",
        "for key, value in pars.items():\n",
        "    params_dict[key] = torch.tensor(value, requires_grad=True)    \n",
        "\n",
        "output_dist = start_SOGA(cfg, params_dict)\n",
        "\n",
        "#optimize(params_dict, neg_log_likelihood, Y, cfg, steps=20)\n",
        "\n",
        "#predictive mean\n",
        "#y_pred = params_dict['muw'].detach().numpy()*X.detach().numpy()+params_dict['mub'].detach().numpy()\n",
        "\n",
        "#predictive variance\n",
        "#sigma_y_pred = np.sqrt(params_dict['sigmay'].detach().numpy()**2 + (X.detach().numpy()*params_dict['sigmaw'].detach().numpy())**2 + params_dict['sigmab'].detach().numpy()**2)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "simple_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
