{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Case Study 5: Bayesian Neural Network\n",
        "\n",
        "Adapted from https://num.pyro.ai/en/stable/examples/bnn.html , we first see the NumPyro implementation and then SOGA.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sogaPreprocessor import *\n",
        "from producecfg import *\n",
        "from libSOGA import *\n",
        "from time import time\n",
        "\n",
        "torch.set_default_dtype(torch.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "sample: 100%|██████████| 3000/3000 [00:03<00:00, 886.01it/s, 15 steps of size 1.86e-01. acc. prob=0.78] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
            "  prec_obs      1.01      0.29      0.99      0.53      1.48   1459.66      1.00\n",
            "   w1[0,0]     -0.18      1.01     -0.24     -1.84      1.43   1153.05      1.00\n",
            "   w1[0,1]     -0.18      0.97     -0.18     -1.74      1.52    628.04      1.00\n",
            "   w1[1,0]     -0.02      0.95     -0.02     -1.62      1.44    841.15      1.00\n",
            "   w1[1,1]     -0.03      0.93     -0.04     -1.40      1.64    903.38      1.00\n",
            "   w2[0,0]     -0.14      0.96     -0.13     -1.80      1.38    966.95      1.00\n",
            "   w2[0,1]     -0.16      1.01     -0.18     -1.71      1.58    938.67      1.00\n",
            "   w2[1,0]     -0.10      0.92     -0.10     -1.58      1.49   1109.30      1.00\n",
            "   w2[1,1]     -0.09      0.99     -0.10     -1.71      1.54    711.64      1.00\n",
            "   w3[0,0]     -0.10      0.92     -0.10     -1.53      1.53    657.53      1.00\n",
            "   w3[1,0]     -0.11      0.96     -0.10     -1.72      1.46   1207.36      1.00\n",
            "\n",
            "Number of divergences: 29\n",
            "\n",
            "MCMC elapsed time: 6.218445539474487\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import os\n",
        "import time\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import jax\n",
        "from jax import vmap\n",
        "import jax.numpy as jnp\n",
        "import jax.random as random\n",
        "\n",
        "import numpyro\n",
        "from numpyro import handlers\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS\n",
        "\n",
        "matplotlib.use(\"Agg\")  # noqa: E402\n",
        "\n",
        "\n",
        "# the non-linearity we use in our neural network\n",
        "def nonlin(x):\n",
        "    return jax.nn.relu(x)\n",
        "\n",
        "\n",
        "# a two-layer bayesian neural network with computational flow\n",
        "# given by D_X => D_H => D_H => D_Y where D_H is the number of\n",
        "# hidden units. (note we indicate tensor dimensions in the comments)\n",
        "def model(X, Y, D_H, D_Y=1):\n",
        "    N, D_X = X.shape\n",
        "\n",
        "    # sample first layer (we put unit normal priors on all weights)\n",
        "    w1 = numpyro.sample(\"w1\", dist.Normal(jnp.zeros((D_X, D_H)), jnp.ones((D_X, D_H))))\n",
        "    assert w1.shape == (D_X, D_H)\n",
        "    z1 = nonlin(jnp.matmul(X, w1))  # <= first layer of activations\n",
        "    assert z1.shape == (N, D_H)\n",
        "\n",
        "    # sample second layer\n",
        "    w2 = numpyro.sample(\"w2\", dist.Normal(jnp.zeros((D_H, D_H)), jnp.ones((D_H, D_H))))\n",
        "    assert w2.shape == (D_H, D_H)\n",
        "    z2 = nonlin(jnp.matmul(z1, w2))  # <= second layer of activations\n",
        "    assert z2.shape == (N, D_H)\n",
        "\n",
        "    # sample final layer of weights and neural network output\n",
        "    w3 = numpyro.sample(\"w3\", dist.Normal(jnp.zeros((D_H, D_Y)), jnp.ones((D_H, D_Y))))\n",
        "    assert w3.shape == (D_H, D_Y)\n",
        "    z3 = jnp.matmul(z2, w3)  # <= output of the neural network\n",
        "    assert z3.shape == (N, D_Y)\n",
        "\n",
        "    if Y is not None:\n",
        "        assert z3.shape == Y.shape\n",
        "\n",
        "    # we put a prior on the observation noise\n",
        "    prec_obs = numpyro.sample(\"prec_obs\", dist.Normal(0., 1.0))  #Originally Gamma(3.0, 1.0)\n",
        "    sigma_obs = 1.0 / jnp.sqrt(prec_obs)\n",
        "\n",
        "    # observe data\n",
        "    with numpyro.plate(\"data\", N):\n",
        "        # note we use to_event(1) because each observation has shape (1,)\n",
        "        numpyro.sample(\"Y\", dist.Normal(z3, sigma_obs).to_event(1), obs=Y)\n",
        "\n",
        "\n",
        "# helper function for HMC inference\n",
        "def run_inference(model, rng_key, X, Y, D_H):\n",
        "    start = time.time()\n",
        "    kernel = NUTS(model)\n",
        "    mcmc = MCMC(\n",
        "        kernel,\n",
        "        num_warmup=1000,\n",
        "        num_samples=2000,\n",
        "        num_chains=1,\n",
        "        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n",
        "    )\n",
        "    mcmc.run(rng_key, X, Y, D_H)\n",
        "    mcmc.print_summary()\n",
        "    print(\"\\nMCMC elapsed time:\", time.time() - start)\n",
        "    return mcmc.get_samples()\n",
        "\n",
        "\n",
        "# helper function for prediction\n",
        "def predict(model, rng_key, samples, X, D_H):\n",
        "    model = handlers.substitute(handlers.seed(model, rng_key), samples)\n",
        "    # note that Y will be sampled in the model because we pass Y=None here\n",
        "    model_trace = handlers.trace(model).get_trace(X=X, Y=None, D_H=D_H)\n",
        "    return model_trace[\"Y\"][\"value\"]\n",
        "\n",
        "\n",
        "# create artificial regression dataset\n",
        "def get_data(N=20, D_X=3, sigma_obs=0.05, N_test=500):\n",
        "    D_Y = 1  # create 1d outputs\n",
        "    np.random.seed(0)\n",
        "    X = jnp.linspace(-1, 1, N)\n",
        "    X = jnp.power(X[:, np.newaxis], jnp.arange(D_X))\n",
        "    W = 0.5 * np.random.randn(D_X)\n",
        "    Y = jnp.dot(X, W) + 0.5 * jnp.power(0.5 + X[:, 1], 2.0) * jnp.sin(4.0 * X[:, 1])\n",
        "    Y += sigma_obs * np.random.randn(N)\n",
        "    Y = Y[:, np.newaxis]\n",
        "    Y -= jnp.mean(Y)\n",
        "    Y /= jnp.std(Y)\n",
        "\n",
        "    assert X.shape == (N, D_X)\n",
        "    assert Y.shape == (N, D_Y)\n",
        "\n",
        "    X_test = jnp.linspace(-1.3, 1.3, N_test)\n",
        "    X_test = jnp.power(X_test[:, np.newaxis], jnp.arange(D_X))\n",
        "\n",
        "    return X, Y, X_test\n",
        "\n",
        "\n",
        "args = [20, 2, 2]\n",
        "N, D_X, D_H = args\n",
        "X, Y, X_test = get_data(N=N, D_X=D_X)\n",
        "\n",
        "# do inference\n",
        "rng_key, rng_key_predict = random.split(random.PRNGKey(0))\n",
        "samples = run_inference(model, rng_key, X, Y, D_H)\n",
        "\n",
        "# predict Y_test at inputs X_test\n",
        "vmap_args = (\n",
        "    samples,\n",
        "    random.split(rng_key_predict, 2000 * 1),\n",
        ")\n",
        "predictions = vmap(\n",
        "    lambda samples, rng_key: predict(model, rng_key, samples, X_test, D_H)\n",
        ")(*vmap_args)\n",
        "predictions = predictions[..., 0]\n",
        "\n",
        "# compute mean prediction and confidence interval around median\n",
        "mean_prediction = jnp.mean(predictions, axis=0)\n",
        "percentiles = np.percentile(predictions, [5.0, 95.0], axis=0)\n",
        "\n",
        "# make plots\n",
        "fig, ax = plt.subplots(figsize=(8, 6), constrained_layout=True)\n",
        "\n",
        "# plot training data\n",
        "ax.plot(X[:, 1], Y[:, 0], \"kx\")\n",
        "# plot 90% confidence level of predictions\n",
        "ax.fill_between(\n",
        "    X_test[:, 1], percentiles[0, :], percentiles[1, :], color=\"lightblue\"\n",
        ")\n",
        "# plot mean prediction\n",
        "ax.plot(X_test[:, 1], mean_prediction, \"blue\", ls=\"solid\", lw=2.0)\n",
        "ax.set(xlabel=\"X\", ylabel=\"Y\", title=\"Mean predictions with 90% CI\")\n",
        "\n",
        "plt.savefig(\"bnn_plot.pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.0, -0.8947368860244751, -0.7894736528396606, -0.6842105388641357, -0.5789473056793213, -0.4736841917037964, -0.3684210777282715, -0.2631578743457794, -0.15789473056793213, -0.05263158679008484, 0.05263161659240723, 0.15789473056793213, 0.26315784454345703, 0.3684210777282715, 0.4736841917037964, 0.5789474248886108, 0.6842105388641357, 0.7894736528396606, 0.8947368860244751, 1.0]\n",
            "(20,)\n"
          ]
        }
      ],
      "source": [
        "print((X[:,1]).tolist())\n",
        "print(X[:,1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def optimize(params_dict, loss_function, y, cfg, steps=500):\n",
        "    optimizer = torch.optim.Adam([params_dict[key] for key in params_dict.keys()], lr=1)\n",
        "\n",
        "    total_start = time()\n",
        "\n",
        "    for i in range(steps):\n",
        "\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        \n",
        "        # loss\n",
        "        current_dist = start_SOGA(cfg, params_dict)\n",
        "\n",
        "        loss = loss_function(y, current_dist)\n",
        "\n",
        "        # Backpropagate\n",
        "        loss.backward(retain_graph=True)\n",
        "        \n",
        "        optimizer.step()\n",
        "\n",
        "        # Print progress\n",
        "        if i % 10 == 0:\n",
        "            out = ''\n",
        "            for key in params_dict.keys():\n",
        "                out = out + key + ': ' + str(params_dict[key].item()) + ' '\n",
        "            out = out + f\" loss: {loss.item()}\"\n",
        "            print(out)\n",
        "\n",
        "    total_end = time()\n",
        "\n",
        "    print('Optimization performed in ', round(total_end-total_start, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mean_squared_error(y_true, dist):\n",
        "    return torch.mean((y_true - dist.gm.mean()) ** 2)\n",
        "\n",
        "def mean_squared_error_bayes(y_true, dist):\n",
        "    #This works for the means but of course not for the variances\n",
        "    return torch.mean((y_true - dist.gm.mean()[:-2]) ** 2)\n",
        "\n",
        "def neg_log_likelihood(y_true, dist):\n",
        "    #Calculate the log-likelihood of the data given the distribution\n",
        "    neg_log_likelihood = 0\n",
        "    for i in range(len(dist.gm.mean())-2):\n",
        "        neg_log_likelihood -= torch.log(dist.gm.marg_pdf(y_true[i].unsqueeze(0), i))\n",
        "    return neg_log_likelihood"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-1.0, -0.8947368860244751, -0.7894736528396606, -0.6842105388641357, -0.5789473056793213, -0.4736841917037964, -0.3684210777282715, -0.2631578743457794, -0.15789473056793213, -0.05263158679008484, 0.05263161659240723, 0.15789473056793213, 0.26315784454345703, 0.3684210777282715, 0.4736841917037964, 0.5789474248886108, 0.6842105388641357, 0.7894736528396606, 0.8947368860244751, 1.0]\n",
            "(20,)\n"
          ]
        }
      ],
      "source": [
        "print((X[:,1]).tolist())\n",
        "print(X[:,1].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m pars\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     11\u001b[0m     params_dict[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(value, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)    \n\u001b[1;32m---> 13\u001b[0m output_dist \u001b[38;5;241m=\u001b[39m start_SOGA(cfg, params_dict)\n",
            "File \u001b[1;32mc:\\Users\\birik\\github\\DeGAS\\src\\libSOGA.py:55\u001b[0m, in \u001b[0;36mstart_SOGA\u001b[1;34m(cfg, params_dict, pruning, Kmax, parallel, useR)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# executes SOGA on nodes on exec_queue\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(exec_queue)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m---> 55\u001b[0m     SOGA(exec_queue\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;241m0\u001b[39m), data, parallel, exec_queue, params_dict)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# returns output distribution\u001b[39;00m\n\u001b[0;32m     58\u001b[0m p, current_dist \u001b[38;5;241m=\u001b[39m merge(cfg\u001b[38;5;241m.\u001b[39mnode_list[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlist_dist)\n",
            "File \u001b[1;32mc:\\Users\\birik\\github\\DeGAS\\src\\libSOGA.py:134\u001b[0m, in \u001b[0;36mSOGA\u001b[1;34m(node, data, parallel, exec_queue, params_dict)\u001b[0m\n\u001b[0;32m    130\u001b[0m     current_trunc \u001b[38;5;241m=\u001b[39m negate(current_trunc) \n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# if parallel is not None and parallel >1:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m#     p, current_dist = parallel_truncate(current_dist, current_trunc, data, parallel)   \u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m p, current_dist \u001b[38;5;241m=\u001b[39m truncate(current_dist, current_trunc, data, params_dict)     \u001b[38;5;66;03m### see libSOGAtruncate\u001b[39;00m\n\u001b[0;32m    135\u001b[0m current_trunc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    136\u001b[0m current_p \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m*\u001b[39mcurrent_p\n",
            "File \u001b[1;32mc:\\Users\\birik\\github\\DeGAS\\src\\libSOGAtruncate.py:361\u001b[0m, in \u001b[0;36mtruncate\u001b[1;34m(dist, trunc, data, params_dict)\u001b[0m\n\u001b[0;32m    359\u001b[0m trunc_rule \u001b[38;5;241m=\u001b[39m trunc_parse(dist\u001b[38;5;241m.\u001b[39mvar_list, trunc, data, params_dict)\n\u001b[0;32m    360\u001b[0m trunc_func \u001b[38;5;241m=\u001b[39m trunc_rule\u001b[38;5;241m.\u001b[39mfunc\n\u001b[1;32m--> 361\u001b[0m norm_fact, new_dist \u001b[38;5;241m=\u001b[39m trunc_func(dist)\n\u001b[0;32m    362\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m norm_fact, new_dist\n",
            "File \u001b[1;32mc:\\Users\\birik\\github\\DeGAS\\src\\libSOGAtruncate.py:106\u001b[0m, in \u001b[0;36mineq_func\u001b[1;34m(self, dist)\u001b[0m\n\u001b[0;32m    101\u001b[0m     b[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ineq_const   \n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# STEP 3: compute moments in the transformed coordinates\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# some components might have 0 prob in the truncation\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;66;03m# indexes contains the indexes of the components that have non-zero probability\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m new_P, new_transl_mu, new_transl_sigma, indexes \u001b[38;5;241m=\u001b[39m compute_moments(transl_mu, transl_sigma, a, b)\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# if the whole distribution has zero prob in the truncation\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indexes) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\birik\\github\\DeGAS\\src\\libSOGAtruncate.py:430\u001b[0m, in \u001b[0;36mcompute_moments\u001b[1;34m(mu, sigma, a, b, idx)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trunc:\n\u001b[0;32m    428\u001b[0m     \u001b[38;5;66;03m# returns the moments for the distribution of dimension n-1, in which the trunc_idx component has been removed\u001b[39;00m\n\u001b[0;32m    429\u001b[0m     muj \u001b[38;5;241m=\u001b[39m compute_lower_mom(mu[indexes], sigma[indexes], a, b, trunc)\n\u001b[1;32m--> 430\u001b[0m     new_sigma \u001b[38;5;241m=\u001b[39m compute_mom2(mu[indexes], sigma[indexes], a, b, trunc, new_P[indexes], new_mu, muj)\n\u001b[0;32m    431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# and_func\u001b[39;00m\n\u001b[0;32m    432\u001b[0m     new_sigma \u001b[38;5;241m=\u001b[39m compute_mom2_and(mu[indexes], sigma[indexes], a, b, new_P[indexes], new_mu, idx)\n",
            "File \u001b[1;32mc:\\Users\\birik\\github\\DeGAS\\src\\libSOGAtruncate.py:490\u001b[0m, in \u001b[0;36mcompute_mom2\u001b[1;34m(mu, sigma, a, b, trunc, new_P, new_mu, muj)\u001b[0m\n\u001b[0;32m    488\u001b[0m     C[:,:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mnorm\u001b[38;5;241m.\u001b[39mlog_prob(b[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mexp()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m(b[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39me0)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mhstack((torch\u001b[38;5;241m.\u001b[39mones((c,\u001b[38;5;241m1\u001b[39m)), muj))\n\u001b[0;32m    489\u001b[0m \u001b[38;5;66;03m# computes the new matrix\u001b[39;00m\n\u001b[1;32m--> 490\u001b[0m new_sigma \u001b[38;5;241m=\u001b[39m new_P\u001b[38;5;241m.\u001b[39mview(c,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mmatmul(mu\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m), new_mu\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(sigma, C\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    491\u001b[0m new_sigma \u001b[38;5;241m=\u001b[39m new_sigma\u001b[38;5;241m/\u001b[39mnew_P\u001b[38;5;241m.\u001b[39mview(c,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(new_mu\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m), new_mu\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_sigma\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "compiledFile=compile2SOGA('../programs/SOGA/Optimization/Case Studies/bnn3.soga')\n",
        "cfg = produce_cfg(compiledFile)\n",
        "\n",
        "#pars = {'mu100':0., 'sigma100':1., 'mu101':0., 'sigma101':1.,'mu110':0., 'sigma110':1.,'mu111':0., 'sigma111':1.,'mu200':0., 'sigma200':1.,\n",
        "        #'mu201':0., 'sigma201':1.,'mu210':0., 'sigma210':1.,'mu211':0., 'sigma211':1.,'mu300':0., 'sigma300':1.,'mu310':0., 'sigma310':1.,}\n",
        "\n",
        "\n",
        "pars = {'mu100':0., 'sigma100':1., 'mu101':0., 'sigma101':1.,'mu110':0., 'sigma110':1.,'mu111':0., 'sigma111':1.,'mu300':0., 'sigma300':1.,'mu310':0., 'sigma310':1.,}\n",
        "params_dict = {}\n",
        "for key, value in pars.items():\n",
        "    params_dict[key] = torch.tensor(value, requires_grad=True)    \n",
        "\n",
        "output_dist = start_SOGA(cfg, params_dict)\n",
        "\n",
        "#optimize(params_dict, neg_log_likelihood, Y, cfg, steps=20)\n",
        "\n",
        "#predictive mean\n",
        "#y_pred = params_dict['muw'].detach().numpy()*X.detach().numpy()+params_dict['mub'].detach().numpy()\n",
        "\n",
        "#predictive variance\n",
        "#sigma_y_pred = np.sqrt(params_dict['sigmay'].detach().numpy()**2 + (X.detach().numpy()*params_dict['sigmaw'].detach().numpy())**2 + params_dict['sigmab'].detach().numpy()**2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
